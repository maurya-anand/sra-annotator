#!/usr/bin/env python3

import os
import requests
from requests.exceptions import HTTPError
import time
import urllib.parse
import re
import json
import argparse
import xmltodict
import pandas
import re


cwd = os.getcwd()
cwd = os.path.abspath(cwd)


def validate_file(f):
    if not os.path.exists(f):
        # Argparse uses the ArgumentTypeError to give a rejection message like:
        # # error: argument input: x does not exist
        raise argparse.ArgumentTypeError("File {0} does not exist".format(f))
    return f


parser = argparse.ArgumentParser(add_help=False,description='''
About:   A command-line tool for retrieving annotations from the NCBI SRA database.
Author:  Anand Maurya
Version: 1.2.0

''',formatter_class=argparse.RawTextHelpFormatter, epilog='''additional information:
    Uses NCBI's Entrez Programming Utilities under the hood to query and retrieve Run-level and Sample-level annotation from the SRA database.
    Learn more about E-Utilities here: https://www.ncbi.nlm.nih.gov/books/NBK25499/
    
having issues?
    Check your internet connection.
    Find the "failed_to_parse.txt" file in the output directory. It contains a list of run accessions that most likely have incorrect or missing annotation in SRA.

''')


# using this order of declaration in order to group them in the help message
optional = parser.add_argument_group('OPTIONAL')
required = parser.add_argument_group('REQUIRED')
addnl_flag = parser.add_mutually_exclusive_group()
parser._optionals.title = "POSITIONAL"


required.add_argument("query", help='''Query string to search the SRA.
Please use quotes '' if the query contains multiple words. eg:
    PRJNA868738
    SRR15736787
    'PRJNA761299 OR SRR15736787'
    'trna[Text Word] AND "Danio rerio"[Organism]'
    '(2008[Publication Date] : 2009[Publication Date]) AND "arabidopsis thaliana"[Organism]'
Please refer https://www.ncbi.nlm.nih.gov/sra/docs/srasearch/ to learn more about basic and advanced search in NCBI SRA.

''')


required.add_argument("-q", "--query", action="store_true", help='''List query SRA accessions in plain text file.
Please make sure that the file contains one accession per line. 
''')
# "-q", "--treat-as-file"


optional.add_argument("-o","--output", type=str, default=cwd, help='''Output directory to store the results. (default: %(default)s)

''')


optional.add_argument("-m","--mode", type=str, choices=['quick','full'], default='quick', help=''''quick' mode dumps the run level annotation, whereas 'full' mode attempts to retrieve both the run level and sample level annotation. 'Full' converts the annotation from JSON to CSV for each run accession.
'Full' mode might not work with all complex queries. (default: %(default)s)

''')


optional.add_argument("-f","--fastq", action='store_true', help='''Locate the web address to the raw data and generate a script to download the fastq file(s). Depending on the number of fastq files to be searched, this can take some time.

''')


addnl_flag.add_argument("-d","--dictionary", required=False, type=validate_file, help='''Use the designated keywords from the JSON file to identify the samples.
Example JSON : { "tissue": [flower, petals], "reagent": "trizol" }

''')


parser.add_argument('-v','--version', action='version', version='%(prog)s 1.2.0')


optional.add_argument("-h", "--help", action='help', default=argparse.SUPPRESS, help='''Show this help message and exit.

'''
)


# reorder the items in the parser._action_groups list, to reorder the groups in the display
parser._action_groups.reverse()
args = parser.parse_args()


esearch_base_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'
esummary_base_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'
elink_base_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi'


failed_runs=[]
completed_runs=[]


output_json_directory=None
output_table_directory=None
output_data_directory=None
output_stats_directory=None


if args.output:
    out_dir = args.output.strip()
    if not os.path.exists(out_dir):
        out_dir = cwd
    else:
        out_dir= os.path.abspath(out_dir)
    output_json_directory=out_dir+'/sra_data/annotation_json'
    output_table_directory=out_dir+'/sra_data/annotation_text'
    output_data_directory=out_dir+'/sra_data/fastq_source'
    output_stats_directory=out_dir+'/sra_data/'


def make_request (url,params) :
    retries = 1
    success = False
    headers = {
        'Content-Type': "application/x-www-form-urlencoded; charset=UTF-8"
    }
    while not success and retries <= 1:
        try:
            response = requests.post(url, data=params, headers=headers)
            response.raise_for_status()
            jsonResponse = response.json(strict=False)
            return (jsonResponse)
            success = True
        except Exception as err:
            print(f'{err}')
            wait = retries * 2
            print (f"Retrying attempt {retries}")
            time.sleep(wait)
            retries+=1


def convert_sra_uids_to_biosample_uids(uids):
    # to prevent conversion of comma to % ( preventing conversion of 15949297,15949296 => 15949297%15949296 in the url)
    elink_params = urllib.parse.urlencode({'dbfrom':'sra','db':'biosample','retmode':'json','id':uids}, safe=',')
    biosample_uids=[]
    # print(elink_base_url,elink_params)
    sra_to_biosample_response = make_request(elink_base_url,elink_params)
    if type(sra_to_biosample_response['linksets']) == list:
        if type(sra_to_biosample_response['linksets'][0]['linksetdbs']) == list:
            if (sra_to_biosample_response['linksets'][0]['linksetdbs'][0]['links']):
                biosample_uids = ",".join(sra_to_biosample_response['linksets'][0]['linksetdbs'][0]['links'])
                return biosample_uids


def write_quick_json(run_to_sam_accn, run_info_items):
    for run in run_to_sam_accn:
        if run in run_info_items:
            combined_run_level_dict={'run':run_info_items[run]}
            json_string = json.dumps(combined_run_level_dict,indent=4)
            print(f"Writing for {run}", end='\r')
            with open(f"{output_json_directory}/{run}.json", "w") as outfile:
                outfile.write(json_string)
                completed_runs.append(run)
        else:
            failed_runs.append(run)


def write_full_json(run_to_sam_accn, run_info_items,sample_info_items):
    for run in run_to_sam_accn:
        if (run in run_info_items) and (run_to_sam_accn[run] in sample_info_items):
            combined_run_level_dict={'run':run_info_items[run],'sample':sample_info_items[run_to_sam_accn[run]]}
            json_string = json.dumps(combined_run_level_dict,indent=4)

            normalized_data = pandas.json_normalize(combined_run_level_dict)
            dataframe = pandas.DataFrame(normalized_data)
            dataframe.insert(0, column = "run", value = run)
            
            print(f"Writing output for {run}", end='\r')
            with open(f"{output_json_directory}/{run}.json", "w") as outfile:
                outfile.write(json_string)
                completed_runs.append(run)
            
            tab_file = f"{output_table_directory}/{run}.csv"
            dataframe.to_csv(tab_file,index=False)
        else:
            failed_runs.append(run)


def parse_sample_info(sample_json):
    sra_accn=None
    sample_info_dict={}
    for id in sample_json:
        if id != 'uids':
            try:
                # using "dict_constructor" parameter to avoid OrderedDict output
                sampledata = xmltodict.parse('<document>'+sample_json[id]['sampledata']+'</document>',dict_constructor=dict)
                identifiers = xmltodict.parse('<document>'+sample_json[id]['identifiers']+'</document>',dict_constructor=dict)
                if identifiers['document']:
                    identifiers_items = re.split(r'; ', identifiers['document'])
                    for ident in identifiers_items:
                        id_type,id_accn = re.split(r': ', ident)
                        if (id_type == 'SRA'):
                            sra_accn = id_accn
                if (sampledata['document']) and (sra_accn is not None):
                    sample_info_dict[sra_accn]=sampledata['document']
            except:
                print(f"Error parsing {id}")
    return (sample_info_dict)


def parse_run_info(run_json):
    run_to_sample_ids={}
    run_info_dict={}
    for id in run_json:
        if id != 'uids':
            try:
                # using "dict_constructor" parameter to avoid OrderedDict output
                expxml = xmltodict.parse('<document>'+run_json[id]['expxml']+'</document>',dict_constructor=dict)
                runs = xmltodict.parse('<document>'+run_json[id]['runs']+'</document>',dict_constructor=dict)
                sample_accn=None
                if expxml['document']['Sample']['@acc']:
                    sample_accn=expxml['document']['Sample']['@acc']
                if type(runs['document']['Run']) == list:
                    for run in runs['document']['Run']:
                        run_to_sample_ids[run['@acc']] = sample_accn
                        run_info_dict[run['@acc']]={'total_spots':run['@total_spots'],'total_bases':run['@total_bases'],'exp_info':expxml}
                else:
                    run_to_sample_ids[runs['document']['Run']['@acc']]=sample_accn
                    run_info_dict[runs['document']['Run']['@acc']]={'total_spots':runs['document']['Run']['@total_spots'],'total_bases':runs['document']['Run']['@total_bases'],'exp_info':expxml}
            except:
                print(f"Error parsing {id}")
    return (run_info_dict,run_to_sample_ids)


def split_list(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def download_sra(run_accn):
    file_url = (f"https://www.ebi.ac.uk/ena/portal/api/filereport?result=read_run&fields=fastq_ftp,sra_ftp&format=JSON&accession={run_accn}")
    out_file_name = (f"{output_data_directory}/{run_accn}.fastq.download.sh")
    if not os.path.exists(out_file_name):
        response = requests.get(file_url, stream = True)
        jsonResponse = response.json(strict=False)
        fastq1=None
        fastq2=None
        fastq1_out=None
        fastq2_out=None
        if isinstance(jsonResponse, list):
            for json_set in jsonResponse:
                if json_set['fastq_ftp']:
                    if ';' in json_set['fastq_ftp']:
                        fastq1,fastq2=json_set['fastq_ftp'].split(';')
                        break
                    else:
                        fastq1 =json_set['fastq_ftp']
            if (fastq1):
                fastq1_out=fastq1.split('/')[-1]
            if (fastq2):
                fastq2_out=fastq2.split('/')[-1]
            
            with open(out_file_name,"w") as download:
                download.write('#!/usr/bin/env bash\n')
                download.write(f"curl -L {fastq1} -o {output_data_directory}/{fastq1_out}\n")
                if (fastq2):
                    download.write(f"curl -L {fastq2} -o {output_data_directory}/{fastq2_out}\n")
        else:
            print ("Unable to locate the raw data. INVALID RESPONSE : ", jsonResponse)


def create_query(term):
    esearch_params = {'db': 'sra','usehistory':'y','retmode':'json','term':term}
    esearch_response = make_request(esearch_base_url,esearch_params)
    if esearch_response['esearchresult']:
        total_result = int(esearch_response['esearchresult']['count'])
        querykey = esearch_response['esearchresult']['querykey']
        webenv = esearch_response['esearchresult']['webenv']
        results_idx = [*range(total_result+1)]
        all_sra_uids =[]
        # chunks of 500 ids => since the index is starting from 0, 'retmax' should be 499
        chunks = list(split_list(results_idx, 500)) 
        # print (f"Found total {total_result} results.")
        print (f"Parsing the response from the server.")
        if total_result > 500:
            print (f"Creating batch queries due to esearch API's limitation.")
        for set, chunk in enumerate(chunks):
            try:
                sra_uids=None
                biosample_uids=None
                sample_json=None
                run_json=None
                # since the chunk index is starting from 0, 'retmax' should be 499
                sra_esummary_params = {'db': 'sra','retmode':'json','query_key':querykey,'WebEnv':webenv,'retstart':chunk[0],'retmax':499}
                sra_esummary_response = make_request(esummary_base_url,sra_esummary_params)
                if sra_esummary_response['result']:
                    run_json = sra_esummary_response['result']
                    sra_uids = ",".join(sra_esummary_response['result']['uids'])
                    if args.mode == 'full':
                        if not os.path.exists(output_json_directory):
                                os.makedirs(output_json_directory)
                        if not os.path.exists(output_table_directory):
                            os.makedirs(output_table_directory)
                        if sra_uids is not None:
                            biosample_uids = convert_sra_uids_to_biosample_uids(sra_uids)
                        if biosample_uids is not None:
                            biosample_esummary_params = urllib.parse.urlencode({'dbfrom':'sra','db':'biosample','retmode':'json','id':biosample_uids}, safe=',')
                            biosample_response = make_request(esummary_base_url,biosample_esummary_params)
                            if (biosample_response['result']):
                                sample_json = biosample_response['result']
                        if run_json is not None and sample_json is not None:
                            runinfo, run_to_sample_ids = parse_run_info(run_json)
                            saminfo = parse_sample_info(sample_json)
                            write_full_json(run_to_sample_ids,runinfo,saminfo)
                            print('')
                            print (f"Finished query set [{set+1}/{len(chunks)}]")
                    else:
                        if run_json is not None:
                            if not os.path.exists(output_json_directory):
                                os.makedirs(output_json_directory)
                            runinfo, run_to_sample_ids = parse_run_info(run_json)
                            write_quick_json(run_to_sample_ids,runinfo)
                            print('')
                            print (f"Finished query set [{set+1}/{len(chunks)}]")
            except:
                print (f"Error generating summary from query set [{set+1}/{len(chunks)}].")


def find_matches (query_term):
    hits_found=[]
    for run in completed_runs:
        with open (f"{output_json_directory}/{run}.json") as json_content:
            for line in json_content:
                if re.search(query_term,line,re.IGNORECASE):
                    if run not in hits_found:
                        hits_found.append(run)
                    break
    return (hits_found)


if args.query:
    if not os.path.exists(args.query):
        q_term = args.query.strip()
        create_query(q_term)
        if (len(failed_runs)>0):
            with open(f"{output_stats_directory}/failed_to_parse.txt", "w") as failed:
                failed.write('\n'.join(failed_runs))
            print(f"Check {output_stats_directory}/failed_to_parse.txt for errors.")
    else:
        inp_ids=[]
        with open(args.query, "r") as list_inp:
            q_list = list_inp.readlines()
            for q_terms in q_list:
                inp_ids.append(q_terms.strip())
        q_term = ' OR '.join(inp_ids)
        create_query(q_term)
        if (len(failed_runs)>0):
            with open(f"{output_stats_directory}/failed_to_parse.txt", "w") as failed:
                failed.write('\n'.join(failed_runs))
            print(f"Check {output_stats_directory}/failed_to_parse.txt for errors.")
    
    if args.dictionary:
        hits_table_file = (f"{output_stats_directory}/keyword_hits.csv")
        patterns=None
        with open(args.dictionary, 'r') as keywords:
            patterns = json.loads(keywords.read())
        hits_dict={}
        for category,qnames in patterns.items():
            if isinstance(qnames, list):
                for word in qnames:
                    if find_matches(word):
                        if not category in hits_dict:
                            hits_dict[category]=find_matches(word)
                        else:
                            hits_dict[category]= hits_dict[category]+find_matches(word)
            elif isinstance(qnames, str):
                if find_matches(qnames):
                    if not category in hits_dict:
                        hits_dict[category]=find_matches(qnames)
                    else:
                        hits_dict[category]= hits_dict[category]+find_matches(qnames)
            else:
                print (f"Nested dictinary not allowed. {qnames}")
        hits_tab_norm = {}
        for header, vals in hits_dict.items():
            hits_tab_norm[header]=list(set(vals))
        hits_tab = pandas.DataFrame.from_dict(hits_tab_norm, orient='index').transpose() 
        hits_tab.to_csv(hits_table_file, index=False,encoding='utf-8',na_rep='-')
        print (f"Samples containing specific keywords can be found in : {hits_table_file}")

    if args.fastq:
        if not os.path.exists(output_data_directory):
            os.makedirs(output_data_directory)
        for run in completed_runs:
            print (f"Locating fastq file(s) for {run}", end='\r')
            try:
                download_sra(run)
            except:
                print (f"Couldn't locate fastq file(s) for {run}", end='\n')
else:
    print (f"Invalid input provided.", end='\n')